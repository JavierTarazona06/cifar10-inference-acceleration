# Jour 1 - D√©marrage + It√©ration I0
## Objectif : Environnement propre + Benchmark latence GPU fiable

---

## üìã Liste des t√¢ches

### Card 1: Configuration de l'environnement
**Statut:** √Ä faire  
**Priorit√©:** üî¥ Critique  
**Assign√© √†:** Javier  
**Due:** J1 matin

- [ ] V√©rifier/installer PyTorch (derni√®re version stable)
- [ ] V√©rifier/installer torchvision
- [ ] V√©rifier CUDA et drivers GPU (version compatible)
- [ ] V√©rifier disponibilit√© de `torch.compile` (PyTorch >= 2.0)
- [ ] Documenter versions utilis√©es (PyTorch, CUDA, GPU model)

**Notes:**
```
python -c "import torch; print(torch.__version__, torch.cuda.is_available())"
```

---

### Card 2: Pr√©paration des donn√©es CIFAR-10
**Statut:** √Ä faire  
**Priorit√©:** üî¥ Critique  
**Assign√© √†:** Javier  
**Due:** J1 matin/midi

- [ ] Impl√©menter chargement CIFAR-10 (train + test)
- [ ] D√©finir transforms **train** (normalisation + augmentations)
- [ ] D√©finir transforms **test** (normalisation uniquement)
- [ ] V√©rifier shapes et classes (10 classes, images 32√ó32)
- [ ] Pr√©parer DataLoader test avec batch_size=1 (pour benchmark)

**Notes:**
- Normalisation : mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616]
- Augmentations recommand√©es : RandomCrop(32, padding=4), RandomHorizontalFlip, RandomRotation

---

### Card 3: Impl√©mentation du benchmark de latence GPU
**Statut:** √Ä faire  
**Priorit√©:** üî¥ Critique  
**Assign√© √†:** Javier  
**Due:** J1 midi/apr√®s-midi

- [ ] Cr√©er fonction `benchmark_latency(model, dataloader, warmup_iters=50, measure_iters=500)`
- [ ] **Entr√©es pr√©calcul√©es sur GPU** (avant boucle, pas de transfert √† chaque it√©ration)
- [ ] **Warm-up** sans mesure (50‚Äì200 it√©rations)
- [ ] **Synchronisation GPU** (`torch.cuda.synchronize()`) avant et apr√®s
- [ ] Boucle de mesure avec `torch.cuda.Event` ou `time.perf_counter()`
- [ ] Calcul de : **moyenne**, **p95** (95e percentile), std, min, max
- [ ] Sauvegarde r√©sultats dans **CSV** (colonnes : variant, accuracy, lat_mean, lat_p95, timestamp)

**Protocole d√©taill√©:**
```python
# Pseudo-code
model.eval()
torch.no_grad()

# Warm-up
for i in range(warmup_iters):
    output = model(input)  # input d√©j√† sur GPU
torch.cuda.synchronize()

# Mesure
times = []
for i in range(measure_iters):
    t0 = perf_counter()
    output = model(input)
    torch.cuda.synchronize()
    t1 = perf_counter()
    times.append((t1 - t0) * 1000)  # en ms

stats = {mean, p95, std}
```

---

### Card 4: Validation du benchmark (stabilit√© & reproductibilit√©)
**Statut:** √Ä faire  
**Priorit√©:** üü† Haute  
**Assign√© √†:** Javier  
**Due:** J1 apr√®s-midi

- [ ] Ex√©cuter benchmark **3 fois** sur un mod√®le dummy (ex. ResNet-18 random weights)
- [ ] V√©rifier variance des r√©sultats (p95 ne doit pas osciller > 10%)
- [ ] Documenter conditions fixes : GPU model, CUDA version, batch size, input shape
- [ ] V√©rifier que `model.eval()` + `torch.no_grad()` sont bien appliqu√©s
- [ ] V√©rifier pas de transferts GPU/CPU dans la boucle
- [ ] G√©n√©rer 1 fichier CSV de r√©sultats test

**Crit√®res d'acceptation :**
- Benchmark produit mean + p95 lisibles
- Variance acceptable (< 10% entre runs)
- CSV bien form√© (colonnes header)

---

### Card 5: Documentation du protocole
**Statut:** √Ä faire  
**Priorit√©:** üü° Moyenne  
**Assign√© √†:** Javier  
**Due:** J1 fin

- [ ] √âcrire README ou section **"Protocole de mesure"** dans notebook
- [ ] Documenter :
  - Warm-up iterations = ?
  - Measure iterations = ?
  - GPU used = ?
  - CUDA synchronization method
  - Batch size = 1
  - Precision = FP32 (par d√©faut J1)
- [ ] Rendre reproductible : versions logg√©es, seed (optionnel)

---

## ‚úÖ Crit√®res d'acceptation J1

- [ ] Script/notebook ex√©cutable avec toutes les √©tapes (env + data + benchmark)
- [ ] Benchmark latence produit **mean + p95** en ms
- [ ] Fichier **CSV r√©sultats** g√©n√©r√©
- [ ] **3 ex√©cutions cons√©cutives** montrent variance acceptable
- [ ] **Protocole document√©** (dans README ou notebook)

---

## üìä Fichiers attendus en sortie

- `bench_latency.py` ou section notebook avec fonction benchmark
- `results_j1_validation.csv` (r√©sultats des 3 runs de validation)
- `README_PROTOCOLE.md` ou √©quivalent (documentation)
