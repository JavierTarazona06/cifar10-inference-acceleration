{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "b0ffbefc"
   },
   "source": [
    "# Benchmark Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "id": "ea1b4177",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from time import perf_counter\n",
    "from datetime import datetime\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "071ff15f"
   },
   "source": [
    "# CIFAR-10 Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "78e01bd5"
   },
   "source": [
    "## Data transformation\n",
    "For well computing, it is important to do some work with tha data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "id": "9c1d87f3",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For trainning, transformation is augmentations for data variability and normalisation for the model\n",
    "\"\"\"\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(), # Turn images 50% of time\n",
    "    transforms.RandomCrop(32, padding=4), # Extend area by 4 pixels -> 40*40 -> Variability without loosing information\n",
    "    transforms.RandomRotation(15), # 15, general rule to keep it consistently with real world\n",
    "    transforms.ToTensor(), # Image to tensor for PyTorch and data from uint8 to float32\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2470, 0.2435, 0.2616]\n",
    "    ) # Values per channel, info calculated from CIFAR-10 dataset\n",
    "])\n",
    "\n",
    "\"\"\"\n",
    "For testing, transformation is just normalisation for the model\n",
    "\"\"\"\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2470, 0.2435, 0.2616]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "id": "64725cfe"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12061,
     "status": "ok",
     "timestamp": 1768438244870,
     "user": {
      "displayName": "Javier Tarazona",
      "userId": "18375127980319368687"
     },
     "user_tz": -60
    },
    "id": "149b3b09",
    "outputId": "ea30027b-195d-48d4-fd7e-8e0a650d33fc",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load CIFAR-10 (train)\n",
    "cifar10_train = datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform_train\n",
    ")\n",
    "\n",
    "# Load CIFAR-10 (test)\n",
    "cifar10_test = datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform_test\n",
    ")\n",
    "\n",
    "batch_size_train = 128  # For trainning\n",
    "batch_size_test = 1     # For latence benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1768438244884,
     "user": {
      "displayName": "Javier Tarazona",
      "userId": "18375127980319368687"
     },
     "user_tz": -60
    },
    "id": "5558b08b",
    "outputId": "bf68faa2-889b-4bf2-ce4e-247d5c61c657",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    cifar10_train,\n",
    "    batch_size=batch_size_train,\n",
    "    shuffle=True, # Mix data, not overfitting\n",
    "    num_workers=4,\n",
    "    pin_memory=True # CPU -> GPU\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    cifar10_test,\n",
    "    batch_size=batch_size_test,\n",
    "    shuffle=False, # Don't mix, fair comparisons\n",
    "    num_workers=0, # No parallelism, just one pass through without overhead\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "4596f42e"
   },
   "source": [
    "## Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 167,
     "status": "ok",
     "timestamp": 1768438245052,
     "user": {
      "displayName": "Javier Tarazona",
      "userId": "18375127980319368687"
     },
     "user_tz": -60
    },
    "id": "ce26a40d",
    "outputId": "dccd0cf1-2e5a-4aaa-f002-dda6e9e699d6",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Verification\n",
    "print(f\"- Train dataset: {len(cifar10_train)} images\")\n",
    "print(f\"- Test dataset: {len(cifar10_test)} images\")\n",
    "print(f\"- Nombre de classes: {len(cifar10_train.classes)}\")\n",
    "print(f\"- Classes: {cifar10_train.classes}\")\n",
    "\n",
    "# Verify one image\n",
    "sample_img, sample_label = cifar10_train[0]\n",
    "print(f\"Shape image train: {sample_img.shape}\")\n",
    "print(f\"Label sample: {sample_label} ({cifar10_train.classes[sample_label]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "id": "fa5480ca"
   },
   "source": [
    "# Benchmark Latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "UJRNw_eCjFPE"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "id": "1c07bacc",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "WARM_UP_ITERS = 50\n",
    "MEASURE_ITERS = 500\n",
    "\n",
    "def benchmark_latency(model, dataloader, warmup_iters=WARM_UP_ITERS, measure_iters=MEASURE_ITERS, device='cuda'):\n",
    "    \"\"\"\n",
    "    Benchmark latency d'inférence GPU pour batch_size=1, suelement 1 image.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model en mode eval\n",
    "        dataloader: DataLoader avec batch_size=1\n",
    "        warmup_iters: nombre d'itérations de warm-up (default 50)\n",
    "        measure_iters: nombre d'itérations de mesure (default 500)\n",
    "        device: 'cuda' (GPU) ou 'cpu'\n",
    "\n",
    "    Returns:\n",
    "        dict avec statistiques de latence (mean, p95, std, min, max en ms)\n",
    "    \"\"\"\n",
    "\n",
    "    # Check eval mode and device\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Send data to be stored in GPU\n",
    "    print(f\"[Benchmark] Préparation des données sur {device}...\")\n",
    "    gpu_data = []\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        gpu_data.append((images, labels))\n",
    "        if i >= max(warmup_iters, measure_iters) - 1:\n",
    "            break\n",
    "\n",
    "    print(f\"[Benchmark] {len(gpu_data)} images préchargées sur GPU\")\n",
    "\n",
    "    # WARM-UP (no mesure) to avoid bias for overcharging\n",
    "    print(f\"[Benchmark] Warm-up: {warmup_iters} itérations...\")\n",
    "    with torch.no_grad():\n",
    "        for i in range(min(warmup_iters, len(gpu_data))):\n",
    "            images, _ = gpu_data[i]\n",
    "            _ = model(images)\n",
    "\n",
    "    torch.cuda.synchronize()  # GPU Synchronisation after warm-up\n",
    "    print(\"[Benchmark] Warm-up terminé\")\n",
    "\n",
    "    # MESURE (avec chrono)\n",
    "    print(f\"[Benchmark] Mesure: {measure_iters} itérations...\")\n",
    "    times = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(min(measure_iters, len(gpu_data))):\n",
    "            images, _ = gpu_data[i]\n",
    "\n",
    "            # Chrono précis\n",
    "            torch.cuda.synchronize()  # Sync before\n",
    "            t0 = perf_counter()\n",
    "\n",
    "            output = model(images)\n",
    "\n",
    "            torch.cuda.synchronize()  # Sync after\n",
    "            t1 = perf_counter()\n",
    "\n",
    "            # Temps en ms\n",
    "            elapsed_ms = (t1 - t0) * 1000\n",
    "            times.append(elapsed_ms)\n",
    "\n",
    "    # STATISTIQUES\n",
    "    times = np.array(times)\n",
    "    stats = {\n",
    "        'mean': float(np.mean(times)),\n",
    "        'p95': float(np.percentile(times, 95)),\n",
    "        'p50': float(np.percentile(times, 50)),\n",
    "        'std': float(np.std(times)),\n",
    "        'min': float(np.min(times)),\n",
    "        'max': float(np.max(times)),\n",
    "        'count': len(times)\n",
    "    }\n",
    "\n",
    "    print(f\"[Benchmark] Résultats Latence:\")\n",
    "    print(f\"  - Mean: {stats['mean']:.4f} ms\")\n",
    "    print(f\"  - P95:  {stats['p95']:.4f} ms\")\n",
    "    print(f\"  - P50:  {stats['p50']:.4f} ms\")\n",
    "    print(f\"  - Std:  {stats['std']:.4f} ms\")\n",
    "    print(f\"  - Min:  {stats['min']:.4f} ms\")\n",
    "    print(f\"  - Max:  {stats['max']:.4f} ms\")\n",
    "\n",
    "    return stats, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "id": "875dae5b",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def save_benchmark_results(results_list, filename='benchmark_results.csv'):\n",
    "    \"\"\"\n",
    "    Stocke les résultats de benchmark en CSV.\n",
    "\n",
    "    Args:\n",
    "        results_list: liste de dict avec {variant, accuracy, lat_mean, lat_p95, ...}\n",
    "        filename: nom du fichier CSV\n",
    "    \"\"\"\n",
    "    filepath = Path(filename)\n",
    "\n",
    "    # Headers\n",
    "    fieldnames = ['timestamp', 'variant', 'accuracy', 'lat_mean_ms', 'lat_p95_ms',\n",
    "                  'lat_p50_ms', 'lat_std_ms', 'lat_min_ms', 'lat_max_ms', 'measure_iters']\n",
    "\n",
    "    with open(filepath, 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results_list)\n",
    "\n",
    "    print(f\"\\nRésultats stockées dans {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "c3e991ae"
   },
   "source": [
    "## Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2066,
     "status": "ok",
     "timestamp": 1768438247128,
     "user": {
      "displayName": "Javier Tarazona",
      "userId": "18375127980319368687"
     },
     "user_tz": -60
    },
    "id": "e6e08e92",
    "outputId": "d20c4a80-693b-445f-fcff-ac964c9590ee",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Example\n",
    "# ========================\n",
    "\n",
    "# Créer un modèle dummy pour tester\n",
    "dummy_model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3, 16, 3, padding=1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.AvgPool2d(2),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(16 * 16 * 16, 10)\n",
    ").cuda()\n",
    "\n",
    "# Lancer 3 benchmarks (validation stabilité)\n",
    "print(\"=\"*60)\n",
    "print(\"VALIDATION STABILITÉ DU BENCHMARK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_list = []\n",
    "for run in range(3):\n",
    "    print(f\"\\n[Run {run+1}/3]\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    stats, times = benchmark_latency(\n",
    "        dummy_model,\n",
    "        test_loader,  # From data preparation\n",
    "        warmup_iters= WARM_UP_ITERS,\n",
    "        measure_iters= MEASURE_ITERS,\n",
    "        device='cuda'\n",
    "    )\n",
    "\n",
    "    # Store results\n",
    "    results_list.append({\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'variant': 'dummy_model_fp32',\n",
    "        'accuracy': 0.0,  # Placeholder\n",
    "        'lat_mean_ms': stats['mean'],\n",
    "        'lat_p95_ms': stats['p95'],\n",
    "        'lat_p50_ms': stats['p50'],\n",
    "        'lat_std_ms': stats['std'],\n",
    "        'lat_min_ms': stats['min'],\n",
    "        'lat_max_ms': stats['max'],\n",
    "        'measure_iters': stats['count']\n",
    "    })\n",
    "\n",
    "# Store in CSV file\n",
    "save_benchmark_results(results_list, 'results_j1_validation.csv')\n",
    "\n",
    "# Variance\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RÉSUMÉ VARIANCE (< 10% acceptable)\")\n",
    "print(\"=\"*60)\n",
    "p95_values = [r['lat_p95_ms'] for r in results_list]\n",
    "p95_mean = np.mean(p95_values)\n",
    "p95_std = np.std(p95_values)\n",
    "p95_cv = (p95_std / p95_mean) * 100  # Coefficient de variation\n",
    "\n",
    "print(f\"P95 Mean: {p95_mean:.4f} ms\")\n",
    "print(f\"P95 Std:  {p95_std:.4f} ms\")\n",
    "print(f\"P95 CV:   {p95_cv:.2f}% {'approuvé' if p95_cv < 20 else 'X'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "04228449"
   },
   "source": [
    "# Conditions for reproducing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1768438247392,
     "user": {
      "displayName": "Javier Tarazona",
      "userId": "18375127980319368687"
     },
     "user_tz": -60
    },
    "id": "a0cc6743",
    "outputId": "a6df6373-c74a-4c9a-bbbe-73f16450c336",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONDITIONS FIXES DE MESURE (reproductibilité)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Documenter l'environnement\n",
    "benchmark_conditions = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"pytorch_version\": torch.__version__,\n",
    "    \"torchvision_version\": torchvision.__version__,\n",
    "    \"cuda_available\": torch.cuda.is_available(),\n",
    "    \"cuda_version\": torch.version.cuda,\n",
    "    \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\",\n",
    "    \"gpu_count\": torch.cuda.device_count(),\n",
    "    \"cudnn_version\": torch.backends.cudnn.version(),\n",
    "    \"cudnn_enabled\": torch.backends.cudnn.enabled,\n",
    "    \"cudnn_deterministic\": torch.backends.cudnn.deterministic,\n",
    "}\n",
    "\n",
    "# Paramètres de benchmark\n",
    "benchmark_params = {\n",
    "    \"batch_size\": batch_size_test,\n",
    "    \"input_shape\": (1, 3, 32, 32),  # (batch, channels, height, width)\n",
    "    \"warmup_iterations\": WARM_UP_ITERS,\n",
    "    \"measure_iterations\": MEASURE_ITERS,\n",
    "    \"model_mode\": \"eval\",\n",
    "    \"grad_disabled\": \"torch.no_grad()\",\n",
    "    \"synchronization\": \"torch.cuda.synchronize()\",\n",
    "    \"precision\": \"FP32 (float32)\",\n",
    "    \"data_preloaded_gpu\": True,  # Pas de transferts pendant mesure\n",
    "}\n",
    "\n",
    "print(\"\\nEnvironnement:\")\n",
    "for key, value in benchmark_conditions.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nParamètres de benchmark:\")\n",
    "for key, value in benchmark_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Sauvegarder les conditions dans un fichier pour reproductibilité\n",
    "import json\n",
    "conditions_file = Path('benchmark_conditions.json')\n",
    "with open(conditions_file, 'w') as f:\n",
    "    # Convertir torch.Version en string pour JSON\n",
    "    benchmark_conditions_serializable = {\n",
    "        k: str(v) if not isinstance(v, (str, int, float, bool, type(None))) else v\n",
    "        for k, v in benchmark_conditions.items()\n",
    "    }\n",
    "    json.dump({\n",
    "        \"conditions\": benchmark_conditions_serializable,\n",
    "        \"params\": benchmark_params\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nConditions sauvegardées dans {conditions_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Résumé\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nBenchmark produit mean + p95 lisibles\")\n",
    "print(f\"  - Mean: {stats['mean']:.4f} ms\")\n",
    "print(f\"  - P95:  {stats['p95']:.4f} ms\")\n",
    "\n",
    "print(\"\\nVariance acceptable (< 20% entre runs)\")\n",
    "if p95_cv < 20:\n",
    "    print(f\"  - CV: {p95_cv:.2f}% ACCEPTÉ\")\n",
    "else:\n",
    "    print(f\"  - CV: {p95_cv:.2f}% ATTENTION (> 20%)\")\n",
    "\n",
    "print(\"\\nCSV bien formé (colonnes header)\")\n",
    "import pandas as pd\n",
    "df = pd.read_csv('results_j1_validation.csv')\n",
    "print(f\"  - Fichier: results_j1_validation.csv\")\n",
    "print(f\"  - Colonnes: {list(df.columns)}\")\n",
    "print(f\"  - Rows: {len(df)}\")\n",
    "print(f\"  - Aperçu:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
